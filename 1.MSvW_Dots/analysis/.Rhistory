# Analaysis code for MSvW Dots study by Max Lovell #
# Setup ----
#make working dir same as script file
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Packages ----
list.of.packages <- c("jsonlite", # read current study's raw data
"osfr",     # download schmidt's data
"archive",  # unpack compressed files from previous studies
"foreign",  # Read Schmidt's data
'rmatio',   # read carpenter's data from matlab
'openxlsx', # read file on previous scale measure effects
"mice",     # multiple imputation
"bfrr",     # Bayes Factors and RR search
"viridis"   # colour palette
)
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(list.of.packages, require, character.only = TRUE)
source("fit_meta_d_MLE.R")
source("fit_meta_d_mcmc.R")
#might also need these:
#install.packages("devtools")
#devtools::install_github("debruine/bfrr")
# Helpers -----
#names
pnames <- c("TMS"="TMS-D", "SOW"="SOW", "SOW_F"="SOW 1st", "SOW_S"="SOW 2nd",
"PHQ"="PHQ-8", "GAD"="GAD-7", "RRS"="RRS",
"RRS_D"="RRS Depression", "RRS_B"="RRS Brooding", "RRS_R"="RRS Rumination",
"WBSI"="WBSI", "WBSI_sup"="WBSI Supression", "WBSI_int"="WBSI Intrusion",
"acc"="Accuracy", "dp"="d'", "c"="c", "adj_dp"="Adj. d'",
"adj_meta_d"="Adj. meta-d'", "H_meta_d"="Hmeta-d", "H_M_ratio"="HM-ratio",
"ln_M_ratio"="log(M-ratio)")
shortlist <- c("TMS","SOW_F","SOW_S","PHQ","GAD","RRS","WBSI","dp","H_M_ratio")
shortlist_group <-c("TMS","SOW_F","SOW_S","PHQ","GAD","RRS","WBSI","adj_meta_d","H_M_ratio")
#errors
se <- function(x,na.rm=F){
if(na.rm){x<-x[!is.na(x)]}
sd(x)/sqrt(length(x))
}
ci <- function(x){ se(x)*qnorm(.975) }
#Short pivot function
widen <- function(dataset,id_cols,names_from,values_from){ # dataset=mfn;id_cols=c("Id","Grupo_");names_from="Session__";values_from=c('dprime','mdDiff',"mdRatio",'ln_mdRatio')
dataset <- data.frame(dataset)
#select id cols
id_comb <- do.call(paste, dataset[id_cols])
id_idx <- which(!duplicated(id_comb))
short <- dataset[id_idx,id_cols]
#create new cols
pivot_rows <- do.call(paste, dataset[names_from])
pivot_dat <- sapply(dataset[values_from],tapply,pivot_rows,function(y) y)
pivot_bind <- do.call(cbind,pivot_dat)
colnames(pivot_bind) <- gsub(" ","",paste(rep(colnames(pivot_dat),each=length(rownames(pivot_dat))),rownames(pivot_dat),sep="_"))
#combine
cbind(short,pivot_bind)
}
round2 <- function(x, digits = 2) {  # Function to always round 0.5 up
posneg <- sign(x)
z <- abs(x) * 10^digits
z <- z + 0.5
z <- trunc(z)
z <- z / 10^digits
z * posneg
}
adj <- function(dataset,outcome,covariates,condition_var='condition_pre'){#dataset<-data_c;outcome<-'dp_diff';covariates<-'c_diff'
eq <- paste0(outcome,"~",condition_var,"+",paste(covariates,collapse='+'))
anc <- summary(lm(eq,data=dataset))
anc$coefficients[2,1:2]
}
# SDT function ----------------
SDT <- function(dataset,resps=c('l','r'),ML=F){ # dataset<-dots_task
#Experimentor error: confidence scale of 0-100 for some subjects
print(paste0('N lost to conf scale size: ',length(unique(dataset[dataset$conf %in% (0:10*10),"ID"]))))
dataset <- dataset[!dataset$conf %in% (0:10*10),]
dataset$ID <- droplevels(dataset$ID)
#pesudoT1 for meta-d'
S1_R1 <- table(dataset[dataset$stim==resps[1]&dataset$resp==resps[1],c("ID","conf")])
S1_R2 <- table(dataset[dataset$stim==resps[1]&dataset$resp==resps[2],c("ID","conf")])
S2_R1 <- table(dataset[dataset$stim==resps[2]&dataset$resp==resps[1],c("ID","conf")])
S2_R2 <- table(dataset[dataset$stim==resps[2]&dataset$resp==resps[2],c("ID","conf")])
#reverse
S1_R1 <- S1_R1[,ncol(S1_R1):1]
S2_R1 <- S2_R1[,ncol(S2_R1):1]
#create dataset
nr_sx <- cbind(S1_R1,S1_R2,S2_R1,S2_R2)
nconf <- ncol(nr_sx)/4
colnames(nr_sx) <- c(paste(rep(c("S1_R1","S1_R2","S2_R1","S2_R2"),each=nconf),rep(c(nconf:1,1:nconf),2),sep="_"))
#adjust for 0 counts
nr_sx_adj <- nr_sx+.5 #from Hmeta-d is adjusted by (1/(n_rat*2)) instead - should standardise this?
#t1 dp and c
colnames(nr_sx_adj)
FAR <- apply(nr_sx_adj[,(nconf+1):(nconf*2)],1,sum) / apply(nr_sx_adj[,1:(nconf*2)],1,sum)
HR <- apply(nr_sx_adj[,(nconf*3+1):(nconf*4)],1,sum) / apply(nr_sx_adj[,(nconf*2+1):(nconf*4)],1,sum)
dp <- qnorm(HR) - qnorm(FAR)
c <- -.5*(qnorm(HR)+qnorm(FAR))
#combine
output <- data.frame(dp=dp,c=c) #dataframe for easier column assignment below
#Meta-d'
mcmc_params <- list(response_conditional=0,estimate_dprime=0,nchains=3,
nadapt=1000,nburnin=1000,nsamples=10000,nthin=1,
dic=0,rhat=0,parallel=1,monitorparams=0,saveallsamples=0,
estimate_mratio=0)
if(ML) {
for(i in 1:nrow(nr_sx)){ # i<-1
output[i,c("ML_meta_d","ML_M_ratio")] <- fit_meta_d_MLE(nr_sx_adj[i,grep('S1',colnames(nr_sx))],nr_sx_adj[i,grep('S2',colnames(nr_sx))])[c('meta_da','M_ratio')]
}
output$ln_M_ratio <- log(output$ML_M_ratio)
} else {
for(i in 1:nrow(nr_sx)){ # i<-1
fit <- fit_meta_d_mcmc(nr_sx[i,grep('S1',colnames(nr_sx))],nr_sx[i,grep('S2',colnames(nr_sx))],mcmc_params)
output[i,c("d1","c1","H_meta_d","H_M_ratio")] <- fit[c("d1","c1","meta_d","M_ratio")]
}
output$ln_M_ratio <- log(output$H_M_ratio)
}
#Add ID and time
output$ID <- rownames(nr_sx)
return(output)
}
################################################################### Compile ----
# Dots Task -------------------
dotsData <- function(){ #note 96831 and 77712 are corrupted, hence warnings
task_files <- list.files("./task_data")
##Vanilla JS files
vjs_files <- task_files[grep("_p(re|ost).json$",task_files)]
task <- c()
for(file_n in vjs_files){ #file_n<-vjs_files[10]
file <- fromJSON(paste0("task_data/",file_n))
if(nrow(file)<194){next}
file$ID <- sub("\\D+","",file_n)
task <- rbind(task,file)
}
#simplify
task <- task[task$confidence!=-1,c("ID","target","response","confidence")] #delete practice trials
task[,c("target","response")] <- apply(task[,c("target","response")],2,function(x){substr(x,1,1)})
names(task)<-c("ID","stim","resp","conf")
##JSPsych files
jsp_files <- task_files[grep("_short.csv$",task_files)]
#options(warn=2)
for(file_n in jsp_files){ # file_n<-jsp_files[1]#"dots_77712_short.csv"
file <- read.table(paste0("task_data/",file_n),header=TRUE,stringsAsFactors=F,sep=",")
file <- file[file$trial_type!="practice",c("trial_type","response","correct_response")]
row.names(file) <- NULL
if(nrow(file)/2<168){next} #note 96831 and 77712 are corrupted, hence warnings
file[!!as.numeric(row.names(file))%%2,"confidence"] <- file[!as.numeric(row.names(file))%%2,"response"]
file <- file[file$trial_type=="experimental",c("response","correct_response","confidence")]
file$ID <- gsub("[a-z_.]","\\1",file_n)
file <- file[,c("ID","correct_response","response","confidence")]
names(file) <- c("ID","stim","resp","conf")
file$stim <- ifelse(file$stim=="e","l","r")
file$resp <- ifelse(file$resp=="e","l","r")
task <- rbind(task,file)
}
task$ID <- factor(task$ID, levels=unique(task$ID)) #ensures they appear in the table() output
return(task)
}
dots_task <- dotsData()
surveyData <- function(){ #file_rgx<-all_rgxs[2]
#NOTE RAW DATA AVAILABLE ON REQUEST
#system('pscp -pw [PASSWORD] [USER EMAIL]:[SERVER PATH]*.csv "[LOCAL PATH]"')
survey_files <- list.files("./survey_data")
all_rgxs <- paste("^metacog",c("pre",paste0("[w|m]",1:10),"post"),"",sep="_")
#merge files
for(file_rgx in all_rgxs){ #file_rgx<-all_rgxs[1] #loop through the files for pre, post, and each day separately # file_rgx<-all_rgxs[1]
day_files <- survey_files[grep(file_rgx,survey_files)] #read in first dataset to add the others to
day_data <- read.csv(paste0("survey_data/",day_files[1]))
day_data <- day_data[3:nrow(day_data),] #remove internal qualtrics headers
for(file_name in 2:length(day_files)){ #loop through the rest and add the rows
file_name <- day_files[file_name]
day_file <- read.csv(paste0("survey_data/",file_name))
day_file <- day_file[3:nrow(day_file),!names(day_file)%in%c("ismobile","px_cm")] #added these to exp later
day_data <- rbind(day_data,day_file) #add file to rest of data for that day
}
suffix <- gsub('\\^|(metacog)|_|(\\[w\\|m\\])','',file_rgx) #add to the end of variables to tag as relevant day
names(day_data)[!grepl("Email",names(day_data))] <- paste(names(day_data)[!grepl("Email",names(day_data))],suffix,sep="_")
#bit of data cleaning, fix variable names, etc
if(file_rgx=="^metacog_pre_"){
print(paste0('original pre-test sample size: ',nrow(day_data)))
dots_pre <- dots_SDT
names(dots_pre) <- paste(names(dots_pre),suffix,sep="_")
names(day_data)[names(day_data)=='email_pre'] <- 'Email'
#pre-clean
print(paste0('n unnfinished responses: ',sum(day_data$Progress_pre!=100 | day_data$Duration..in.seconds._pre<=0 | day_data$Finished_pre!=1 | day_data$DistributionChannel_pre=="preview")))
day_data <- day_data[day_data$Progress_pre==100 & day_data$Duration..in.seconds._pre>0 & day_data$Finished_pre==1 & day_data$DistributionChannel_pre!="preview",]
print(paste0('n with duplicated emails: ',sum(day_data$Email%in%day_data$Email[duplicated(day_data$Email)])))
day_data <- day_data[!day_data$Email%in%day_data$Email[duplicated(day_data$Email)],] #THIS MIGHT NOT BE GOOD IF DATA WAS EVER DOWNLOADED TWICE
print(paste0('n pre-test before task data merge: ',nrow(day_data)))
survey <- merge(day_data,dots_pre, by.x="Random_ID_pre", by.y="ID_pre", all.x=F, all.y=F) #add task data #note technical error means some task data incomplete - remove
print(paste0('n pre-test after task data merge: ',nrow(survey)))
} else {
if(file_rgx=="^metacog_post_"){#fixing some badly named vars in qualtrics:
day_data <- day_data[day_data$Progress_post==100 & day_data$Duration..in.seconds._post>0 & day_data$Finished_post==1 & day_data$DistributionChannel_post!="preview",]
dots_post <- dots_SDT
names(dots_post) <- paste(names(dots_post),suffix,sep="_")
day_data <- merge(day_data,dots_post, by.x="Random_ID_post", by.y="ID_post", all.x=F, all.y=F) #add breath task data
} else if(grepl('([1-9]|10)',file_rgx)){ #rename focus and yesterdays activities questions
colnames(day_data) <- sub('Q2','activities',colnames(day_data))
colnames(day_data) <- sub('Q1_1','focus',colnames(day_data))
colnames(day_data) <- sub('focus_1_1','focus_1',colnames(day_data))
#clean before merge (stops some duplications)
day_data <- day_data[day_data[,grep('Progress',colnames(day_data))]==100,]
day_data <- day_data[day_data[,grep('Finished',colnames(day_data))]==1,]
day_data <- day_data[day_data[,grep('Duration..in.seconds.',colnames(day_data))]>0,]
day_data <- day_data[day_data[,grep('DistributionChannel',colnames(day_data))]!="preview",]
}
names(day_data)[names(day_data)=='RecipientEmail'] <- 'Email'
#merge
survey <- merge(survey,day_data,by="Email",all=T) #add to the other days' data by email
}
}
#Remove duplicate emails: people who completed the same survey more than once.
survey$Email <- tolower(survey$Email)
dups <- survey$Email[duplicated(survey$Email)]
print(paste0('n with duplicated emails: ',length(unique(dups))))
survey <- survey[!survey$Email %in% dups,]
rownames(survey) <- NULL #reset row numbers
#delete identifying information
nonymous <- c('Location','UserAgent','meta_info','IPAddress','Email','email','prolific_id')
survey <- survey[,!grepl(paste(nonymous,collapse='|'),colnames(survey))]
return(survey)
}
survey <- surveyData()
#dots_SDT <- SDT(dots_task)
#saveRDS(dots_SDT,'rds_files/dots_SDT.rds')
#write.csv(dots_SDT,'csv_files/dots_SDT.csv')
dots_SDT <- readRDS('rds_files/dots_SDT.rds')
SDT_vars <- colnames(dots_SDT)[-ncol(dots_SDT)]
survey <- surveyData()
#saveRDS(survey,'rds_files/anonymised_data.rds')
#write.csv(survey,'csv_files/anonymised_data.csv')
survey1 <- readRDS('rds_files/anonymised_data.rds')
all(sort(survey1$Random_ID_pre)==sort(survey$Random_ID_pre))
sort(survey1$Random_ID_pre)
sort(survey$Random_ID_pre)
diff(sort(survey1$Random_ID_pre),sort(survey$Random_ID_pre))
table(sort(survey1$Random_ID_pre),sort(survey$Random_ID_pre))
unique(sort(survey1$Random_ID_pre),sort(survey$Random_ID_pre))
sort(survey1$Random_ID_pre)
sort(survey$Random_ID_pre)
survey
sort(survey1$Random_ID_pre)
sort(survey1$Random_ID_pre)
sort(survey$Random_ID_pre)
unique((survey1$Random_ID_pre))
unique(survey$Random_ID_pre)
unique(survey1$Random_ID_pre)
unique(survey$Random_ID_pre)
unique(survey$Random_ID_pre[survey1$Random_ID_pre!=''])
unique(survey$Random_ID_pre[!is.na(survey$Random_ID_pre)])
unique(survey$Random_ID_pre[!is.na(survey1$Random_ID_pre)])
unique(survey$Random_ID_pre[!is.na(survey$Random_ID_pre)])
unique(survey$Random_ID_pre[!is.na(survey1$Random_ID_pre)])
unique(survey1$Random_ID_pre[!is.na(survey1$Random_ID_pre)])
unique(survey$Random_ID_pre[!is.na(survey$Random_ID_pre)])
survey1$Random_ID_pre
table(survey1$Random_ID_pre)
table(survey$Random_ID_pre)
folder<-'survey_data'
files <- list.files(folder,full.names=T)
emails <- c()
f<-files[1]
#read data
csv <- read.csv(f)
#extract email
email <- csv[-c(1,2),colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')]
#convert to lowercase
email_l <- sapply(email,tolower)
#save lowercase back into original location
csv[-c(1,2),colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')] <- email_l
#save emails in list
emails <- c(emails, email_l)
#get unique emails
emails <- unname(unlist(emails))
emails <- emails[emails!='']
u_emails <- unique(emails)
u_emails
#generate random Ids
random_ids <- c()
while(length(random_ids)<length(u_emails)){
random_ids <- unique(replicate(length(u_emails),paste(sample(c(LETTERS,0:9), 20, replace=T),collapse='')))
}
ref <- cbind(email=u_emails,id=random_ids)
ref
csv <- read.csv(f) #print(which(files==f))
#dataset columns
e_cols <- colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')
#read data
csv <- read.csv(f)
#extract email
email <- csv[-c(1,2),colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')]
#convert to lowercase
email_l <- sapply(email,tolower)
#save lowercase back into original location
csv[-c(1,2),colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')] <- email_l
emails <- c()
#read data
csv <- read.csv(f)
#extract email
email <- csv[-c(1,2),colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')]
#convert to lowercase
email_l <- sapply(email,tolower)
#save lowercase back into original location
csv[-c(1,2),colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')] <- email_l
#dataset columns
e_cols <- colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')
e_cols
email <- unlist(csv[-c(1,2),e_cols]) #extract each email to replace
email
email <- email[email != ""]
email
csv[csv==e]
e<-email[1]
csv[csv==e]
csv
ref[ref[,'email']==e,'id']
f<-files[10]
emails <- c()
#read data
csv <- read.csv(f)
#extract email
email <- csv[-c(1,2),colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')]
#convert to lowercase
email_l <- sapply(email,tolower)
#save lowercase back into original location
csv[-c(1,2),colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')] <- email_l
email_l
f<-files[100]
#read data
csv <- read.csv(f)
#extract email
email <- csv[-c(1,2),colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')]
#convert to lowercase
email_l <- sapply(email,tolower)
email_l
#save lowercase back into original location
csv[-c(1,2),colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')] <- email_l
csv
#save emails in list
emails <- c(emails, email_l)
emails
#get unique emails
emails <- unname(unlist(emails))
emails <- emails[emails!='']
u_emails <- unique(emails)
#generate random Ids
random_ids <- c()
while(length(random_ids)<length(u_emails)){
random_ids <- unique(replicate(length(u_emails),paste(sample(c(LETTERS,0:9), 20, replace=T),collapse='')))
}
ref <- cbind(email=u_emails,id=random_ids)
ref
csv <- read.csv(f) #print(which(files==f))
#dataset columns
e_cols <- colnames(csv) %in% c('Email', 'Email validation', 'RecipientEmail')
email <- unlist(csv[-c(1,2),e_cols]) #extract each email to replace
email <- email[email != ""]
email
e_cols
csv[csv==e]
e<-email[1]
csv[csv==e] <- ref[ref[,'email']==e,'id']
e<-email[1]
e
csv[csv==e]
csv
ref[ref[,'email']==e,'id']
ref[,'email']==e
ref
e
ref[ref[,'email']==e,'id']
